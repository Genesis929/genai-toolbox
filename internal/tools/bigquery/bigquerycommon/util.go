// Copyright 2025 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package bigquerycommon

import (
	"context"
	"fmt"
	"sort"
	"strings"

	bigqueryapi "cloud.google.com/go/bigquery"
	"github.com/googleapis/genai-toolbox/internal/util/parameters"
	bigqueryrestapi "google.golang.org/api/bigquery/v2"
)

// DryRunQuery performs a dry run of the SQL query to validate it and get metadata.
func DryRunQuery(ctx context.Context, restService *bigqueryrestapi.Service, projectID string, location string, sql string, params []*bigqueryrestapi.QueryParameter, connProps []*bigqueryapi.ConnectionProperty) (*bigqueryrestapi.Job, error) {
	useLegacySql := false

	restConnProps := make([]*bigqueryrestapi.ConnectionProperty, len(connProps))
	for i, prop := range connProps {
		restConnProps[i] = &bigqueryrestapi.ConnectionProperty{Key: prop.Key, Value: prop.Value}
	}

	jobToInsert := &bigqueryrestapi.Job{
		JobReference: &bigqueryrestapi.JobReference{
			ProjectId: projectID,
			Location:  location,
		},
		Configuration: &bigqueryrestapi.JobConfiguration{
			DryRun: true,
			Query: &bigqueryrestapi.JobConfigurationQuery{
				Query:                sql,
				UseLegacySql:         &useLegacySql,
				ConnectionProperties: restConnProps,
				QueryParameters:      params,
			},
		},
	}

	insertResponse, err := restService.Jobs.Insert(projectID, jobToInsert).Context(ctx).Do()
	if err != nil {
		return nil, fmt.Errorf("failed to insert dry run job: %w", err)
	}
	return insertResponse, nil
}

// DatasetValidator defines the interface for checking if a dataset is allowed.
type DatasetValidator interface {
	IsDatasetAllowed(projectID, datasetID string) bool
}

// ValidateQueryAgainstAllowedDatasets validates a SQL query against a list of allowed datasets.
// It uses both dry run and a local parser to support authorized views.
func ValidateQueryAgainstAllowedDatasets(
	ctx context.Context,
	restService *bigqueryrestapi.Service,
	projectID string,
	location string,
	sql string,
	params []*bigqueryrestapi.QueryParameter,
	connProps []*bigqueryapi.ConnectionProperty,
	validator DatasetValidator,
) (*bigqueryrestapi.Job, error) {
	dryRunJob, err := DryRunQuery(ctx, restService, projectID, location, sql, params, connProps)
	if err != nil {
		return nil, fmt.Errorf("query validation failed: %w", err)
	}

	if dryRunJob.Statistics == nil || dryRunJob.Statistics.Query == nil {
		return nil, fmt.Errorf("dry run failed to return query statistics")
	}
	statementType := dryRunJob.Statistics.Query.StatementType
	// Common restricted operations
	switch statementType {
	case "CREATE_SCHEMA", "DROP_SCHEMA", "ALTER_SCHEMA":
		return nil, fmt.Errorf("dataset-level operations like '%s' are not allowed when dataset restrictions are in place", statementType)
	case "CREATE_FUNCTION", "CREATE_TABLE_FUNCTION", "CREATE_PROCEDURE":
		return nil, fmt.Errorf("creating stored routines ('%s') is not allowed when dataset restrictions are in place, as their contents cannot be safely analyzed", statementType)
	case "CALL":
		return nil, fmt.Errorf("calling stored procedures ('%s') is not allowed when dataset restrictions are in place, as their contents cannot be safely analyzed", statementType)
	}

	// Use a map to avoid duplicate table names from the dry run result.
	tableIDSet := make(map[string]struct{})
	queryStats := dryRunJob.Statistics.Query
	if queryStats != nil {
		for _, tableRef := range queryStats.ReferencedTables {
			tableIDSet[fmt.Sprintf("%s.%s.%s", tableRef.ProjectId, tableRef.DatasetId, tableRef.TableId)] = struct{}{}
		}
		if tableRef := queryStats.DdlTargetTable; tableRef != nil {
			tableIDSet[fmt.Sprintf("%s.%s.%s", tableRef.ProjectId, tableRef.DatasetId, tableRef.TableId)] = struct{}{}
		}
		if tableRef := queryStats.DdlDestinationTable; tableRef != nil {
			tableIDSet[fmt.Sprintf("%s.%s.%s", tableRef.ProjectId, tableRef.DatasetId, tableRef.TableId)] = struct{}{}
		}
	}

	var violatingTables []string
	for tableID := range tableIDSet {
		parts := strings.Split(tableID, ".")
		if len(parts) == 3 {
			if !validator.IsDatasetAllowed(parts[0], parts[1]) {
				violatingTables = append(violatingTables, tableID)
			}
		}
	}

	if len(tableIDSet) > 0 && len(violatingTables) == 0 {
		return dryRunJob, nil
	}

	// If violations were found, check if they are explicitly in the SQL to support authorized views.
	if len(violatingTables) > 0 {
		explicitlyReferenced, err := IsAnyTableExplicitlyReferenced(sql, projectID, violatingTables)
		if err != nil {
			return nil, fmt.Errorf("failed to analyze query for explicit table references: %w", err)
		}
		if explicitlyReferenced {
			return nil, fmt.Errorf("access to dataset '%s' is not allowed", strings.Join(strings.Split(violatingTables[0], ".")[:2], "."))
		}
	}

	// Fall back to TableParser for final intent verification or if dry run was inconclusive.
	parsedTables, parseErr := TableParser(sql, projectID)
	if parseErr != nil {
		return nil, fmt.Errorf("could not safely analyze query with dataset restrictions: %w", parseErr)
	}

	for _, tableID := range parsedTables {
		parts := strings.Split(tableID, ".")
		if len(parts) == 3 {
			if !validator.IsDatasetAllowed(parts[0], parts[1]) {
				return nil, fmt.Errorf("access to dataset '%s.%s' is not allowed", parts[0], parts[1])
			}
		}
	}

	return dryRunJob, nil
}

// BQTypeStringFromToolType converts a tool parameter type string to a BigQuery standard SQL type string.
func BQTypeStringFromToolType(toolType string) (string, error) {
	switch toolType {
	case "string":
		return "STRING", nil
	case "integer":
		return "INT64", nil
	case "float":
		return "FLOAT64", nil
	case "boolean":
		return "BOOL", nil
	default:
		return "", fmt.Errorf("unsupported tool parameter type for BigQuery: %s", toolType)
	}
}

// InitializeDatasetParameters generates project and dataset tool parameters based on allowedDatasets.
func InitializeDatasetParameters(
	allowedDatasets []string,
	defaultProjectID string,
	projectKey, datasetKey string,
	projectDescription, datasetDescription string,
) (projectParam, datasetParam parameters.Parameter) {
	if len(allowedDatasets) > 0 {
		if len(allowedDatasets) == 1 {
			parts := strings.Split(allowedDatasets[0], ".")
			defaultProjectID = parts[0]
			datasetID := parts[1]
			projectDescription += fmt.Sprintf(" Must be `%s`.", defaultProjectID)
			datasetDescription += fmt.Sprintf(" Must be `%s`.", datasetID)
			datasetParam = parameters.NewStringParameterWithDefault(datasetKey, datasetID, datasetDescription)
		} else {
			datasetIDsByProject := make(map[string][]string)
			for _, ds := range allowedDatasets {
				parts := strings.Split(ds, ".")
				project := parts[0]
				dataset := parts[1]
				datasetIDsByProject[project] = append(datasetIDsByProject[project], fmt.Sprintf("`%s`", dataset))
			}

			var datasetDescriptions, projectIDList []string
			for project, datasets := range datasetIDsByProject {
				sort.Strings(datasets)
				projectIDList = append(projectIDList, fmt.Sprintf("`%s`", project))
				datasetList := strings.Join(datasets, ", ")
				datasetDescriptions = append(datasetDescriptions, fmt.Sprintf("%s from project `%s`", datasetList, project))
			}
			sort.Strings(projectIDList)
			sort.Strings(datasetDescriptions)
			projectDescription += fmt.Sprintf(" Must be one of the following: %s.", strings.Join(projectIDList, ", "))
			datasetDescription += fmt.Sprintf(" Must be one of the allowed datasets: %s.", strings.Join(datasetDescriptions, "; "))
			datasetParam = parameters.NewStringParameter(datasetKey, datasetDescription)
		}
	} else {
		datasetParam = parameters.NewStringParameter(datasetKey, datasetDescription)
	}

	projectParam = parameters.NewStringParameterWithDefault(projectKey, defaultProjectID, projectDescription)

	return projectParam, datasetParam
}
